\section{Design and implementation}

\begin{figure}[t]   %% START_FIGURE
\centerline{\includegraphics[scale=0.4]{./figs/giga-impl-leveldb-clusterfs}}
\vspace{10pt}
\caption{\textit{\footnotesize
Our scalable metadata service integrates two components: GIGA+ \cite{GIGA11},
a highly parallel and load-balanced indexing technique 
to partition metadata over multiple servers, and TableFS \cite{TableFS},
an log-structured (LevelDB) on-disk metadata representation on each server.
This integrated solution is layered on top of an existing cluster
file system deployment (PanFS) to improve metadata
and small file operation efficiency.
}}
%\vspace{10pt}
\hrule
\label{fig:design}
\end{figure}       %% END_FIGURE

Figure \ref{fig:design} presents the overall architecture of our scalable
metadata service. Our metadata service is a middleware inserted into
existing deployments of cluster file systems to improve metadata efficiency
while maintaining high I/O bandwidth for data transfers.
The system uses a client-server architecture,
and consists of three core components:

\begin{itemize}
\item{\textbf{Client:}} Applications interact with our middleware
through the FUSE user-level file system \cite{fuse},
through a library directly linked into the application
or through a module in a common library such as MPI-IO \cite{mpi}.
The stateless client-side code redirects applications' file operations
to the appropriate destination according to the types of operations.
All metadata requests (e.g. \texttt{create()} and \texttt{mkdir()}),
and data requests on small files (e.g. \texttt{read()} and \texttt{write()}),
are handled by the metadata indexing modules that address
these requests to the appropriate server.
For all data operations on large size files, the client code redirects
the request directly to the underlying cluster file system to take full
advantage of data I/O bandwidth. A newly created, but growing file
may be transparently reopened by the client module.

\item{\textbf{Metadata Indexing Server:}}
Each indexing server manages its local metadata storage backend to store and
access all metadata information and small file data.
It uses the GIGA+ algorithm to
partition large directories across indexing servers. It also monitors the growth
of small files, and migrates newly large files into
the underlying cluster file system
when its size exceeds a threshold.

\item{\textbf{Metadata Storage Backend:}}
The metadata storage backend is a modified version of \tfs which packs metadata
and small file data into large, log-structured, flat files,
and stores these files
in the underlying cluster file system. Since \tfs converts random updates into
sequential writes, it greatly improves disk performance. In order to
dynamically redistribute large directories,
the metadata storage backend also modifies
\tfs to support exporting and importing flat files in batch.

\end{itemize}

Remainder of this section describes more details of our system.
Section \ref{design.giga} presents a primer on how \giga distributes metadata.
Section \ref{design.tablefs} shows how \tfs stores all file system metadata
and small files using a single on-disk structure on each server.
Section \ref{design.integration} focus on the challenges in effectively
integrating \giga and \tfs to work with existing cluster file systems.


\subsection{Distributed Metadata Indexing}
\label{design.giga}
\input{giga}

\subsection{Metadata Storage Backend}
\label{design.tablefs}
\input{tablefs}

\subsection{Integrating \giga{} and \tfs{}}
\label{design.integration}

To effectively build \sys middleware that integrates
the \giga distribution mechanism and the \tfs metadata representation,
we have to tackle two main challenges:
modify \tfs to support for
migrating directory partitions as required by \giga;
and decouple metadata and data paths to
achieve high performance for both paths.
This section discusses the modifications we made to
overcome these two challenges.


~\\
\textbf{Metadata representation -- }
\tfs stores all additional metadata including \giga per-directory hash
partitions, entries in each hash partition, and \giga
bootstrapping information such as root partition entry
and \giga configuration state.
The general schema used to store all metadata is:

\begin{table}[!htc]
\center
\vspace{10pt}
\begin{tabular}{c|c}
key & \texttt{parentDirID,gigaPartitionID,hash(dirEntry)} \\
\midrule \\
value & \texttt{attributes, symlink|data|gigaMetaState]} \\
\end{tabular}
\label{tab:keyschema}
\end{table}

The main difference from the \tfs schema described in Section
\ref{design.tablefs} is the addition of two \giga specific fields:
\texttt{gigaPartitionID} to identify a
\giga hash partition within a directory and \texttt{gigaMetaState} to store the
hash partition related mapping information for directories.
These \giga related fields are used only
if a large directory is large enough to be
distributed over multiple metadata servers.
\footnote{A optimization is to eliminate \texttt{gigaPartitionID} in the key
by using the same hash function for both \giga and \tfs keys,
since the hash of entry name can determine the partition ID.}

~\\
\textbf{Partition splitting -- }
Each server's \tfs instance stores metadata,
including \giga directory partitions and their directory
entries in \ldb which stores them as
a set of files (whose format is called SSTable) in a server specific directory
in the underlying cluster file system.
Each \giga server process splits a large partition $P$ on
into itself and another hash partition $P'$ which is managed by a
different server; this split involves migrating approximately half the entries
from old partition $P$ to the new partition $P'$ on another server.
During splitting, the partition in migration is locked against client
for simplification. We explored several ways
to perform this cross-server partition split.

A straightforward solution would be to perform a range scan on
partition $P$, and remove about half the entries
(that will be migrated to the new partition $P'$) from $P$.
All removed entries would then be batched together
and sent in a large RPC message to the server that will manage partition $P'$.
The split receiver would insert each key in the batch into its own \tfs instance.
While simplicity of this solution makes it attractive,
it is slow in practice and vulnerable to failures during splitting.

We have devised a faster and safer technique to
reduce the time that the splitting range is locked.
The immutability of SSTables in \ldb makes a fast bulk insert possible --
an SSTable whose range does not overlap any part of a current LSM tree
can be added to Level 0 without its data being pushed through the
write-ahead log and minor compaction process.
To take advantage of this opportunity, we extended \tfs
to support a three-phase \giga split operation:

\begin{itemize}
\item{Phase 1:} The split initiator locks and
then performs a range scan on its \tfs instance
to find all entries in the hash-range that needs to be moved to another server.
Instead of packing these into an RPC message,
the results of this scan are written in SSTable format to a file in the
underlying cluster file system.

\item{Phase 2:} The split initiator notifies the split receiver about
the path to the SSTable-format split file in a much smaller RPC message.
Since this file is stored in shared storage,
the split receiver directly inserts this file as a symbolic link
into the \ldb tree structure without actually copying this file.
The insertion of this file into the split receiver is the commit
part of the split transaction.

\item{Phase 3:} The final step is a clean-up phase:
after the split receiver completes the bulk insert operation, it notifies the
initiator, who then deletes the migrated key-range from its \tfs instance
and unlocks the range.
\footnote{The three phases of splitting can be refined even further:
Assume that the splitting is initiated at the time $T_{split}$.
The split initiator can generate SSTables containing entries
older than $T_{split}$ without locking the hash range.
When the generation of SSTables with entries older than $T_{split}$ is finished,
the split initiator can lock the hash range and then write SSTables with
newly added or updated entries later than $T_{split}$.
By doing so, the duration of locking splitting hash range can be further reduced.
However, due to the code complexity of this optimization,
we left this optimization for future work.}

\end{itemize}


~\\
\textbf{Decoupled data and metadata path -- }
All metadata and small file operations go to the \giga server;
however, following the same path for data operations on large files
would incur a large and unnecessary performance penalty
from shipping data through a single busy machine and
over the network an extra time.
This penalty can be significant in HPC use-cases
because large files are easily be gigabytes to terabytes
and approaching petabyte in size.

To avoid this penalty our middleware is designed to perform all
data-path operations on large files directly
through the cluster file system module in client machine.
Figure \ref{fig:design} illustrates this data path (BLUE colored lines).
Once the application opens a file with size greater than $T$,
the \sys client library code will get back a symbolic link to the physical
path in the cluster file system, which it will open locally.
All subsequent accesses to this large file will go through
the client operating system to the native cluster file system
in parallel transfers.
Thus applications should achieve the same data bandwidth as
clients access to the underlying cluster file system.

FUSE-based clients can use the same trick but with additional overhead
of double context switching and memory copying \cite{PLFS}.
Recent work \cite{fuseopt} shows that the FUSE kernel module can be modified
to make performance degradation less than $3\%$. We have not implemented
this modification in our current prototype yet, because FUSE is not preferred
for HPC parallel codes, and is mostly used for external user access
and storage management (e.g. $ls$, $mkdir$, $find$).

While a file is open, some of its attributes (e.g., file size and last access time)
may change in the cluster file system relative to
\tfs's pre-open and stale copy of the file's attributes.
\sys monitors what large files are currently open.
For attribute queries to open files, \sys will directly query the underlying
cluster file system to get the most up-to-date attribute values.
Later on or after the file is closed,
\sys will synchronize both copies of these attributes.
\footnote{Some attribute changes, such as permissions, can be updated by \sys servers.}
By doing so, \sys servers can achieve the same level of metadata consistency
as provided by the underlying cluster file system.

~\\
\textbf{Layering on the Panasas file system -- }
In PanFS \cite{PanFS}, the \textit{Volume} is the basic unit
administrators use to manage the storage pool of PanFS.
The volume is a directory hierarchy with a quota limit, and appears
as a directory below the single mount point for the whole storage system.
Each PanFS volume is only managed by a single metadata manager, although
the data of all files created in all volumes are spread over the entire storage pool.

A typical PanFS storage cluster has multiple shelves,
each shelf consisting of one or two metadata managers and 9 or 10 storage nodes.
However, since a volume is assigned to a particular metadata manager,
all the metadata accesses to any file/directory in a volume
can only be served by that single metadata manager.

%To take advantage of multiple metadata managers in PanFS,
%our middleware load balance the storage of SSTables and large files across volumes.
%Assume the case that we run the same number of \sys server processes
%as the available metadata managers in PanFS.
%\sys server processes can be run in the same machines that run PanFS metadata managers,
%or in other machines behaving like proxies.
%For each metadata manager, we create a volume and assign the volume
%to a unique \sys server process.
%The \sys server process then stores all the SSTables,
%and large files in its managed directory partitions to it assigned volume.

Since \sys distributes a single namespace over all \sys servers at fine granularity
(the partition of a directory or an entire small directory),
all the metadata managers of PanFS can be exploited and load balanced
if each is responsible for volumes backing the same number of \sys servers.

The simple way to do this is to run one \sys server per PanFS server and
has the \ldb instance in a \sys server store its SSTables in a volume specific
to that \sys server. Large files can be randomly assigned to
hidden directories in all PanFS volumes.

~\\
~\\
\textbf{Fault tolerance -- }
The middleware design and techniques used for scaling the performance
of \sys does not impose any major new challenges for handling failures.
The primary functions required for fault tolerance include
replication of the \sys server's write-ahead logging for file system
state changes into the storage of another machine,
detection of server failure, failover to a backup server,
and replaying of the replicated write-ahead log.
These are all standard techniques found in parallel file systems like PanFS.
The replication of the write-ahead log may not even be necessary
if \sys is layered on a full-featured file system such as PanFS,
and its write-ahead log is RAIDed or replicated by the underlying file system.

For directory partition splitting and other operations requiring
distributed transactions (e.g.\texttt{ hardlink} and \texttt{rename}),
these are implemented as three-phase operations with failure protection
from write-ahead logging in source and destination server
and eventual garbaging collection of incorrupted, hidden, interrupted states.
\ldb also implements write-ahead logging and atomic batching operation
which is used as primitives for fault tolerance by \tfs.

The library version of the \sys client does not bring any new failure properties
either.
Directory-specific client state recovery is naturally supported by the GIGA+
protocal, which tolerates incomplete or stale state by
re-fetching the current server list and
rebuilding its directory index cache each time it contact specific servers.
For most operations except \texttt{open} and \texttt{close},
the \sys server does not need to keep state about its clients.
Revoking the \texttt{open} reference count for a large file can be achieved by
maintaining renewable lease on the server side,
or by polling the state if the file in the underlying file system
using proprietary APIs, if at all.
The migration of small files from \tfs to the underlying file system is
handled by the \sys server, and is protected by its write-ahead logging
and server failure protocol.
