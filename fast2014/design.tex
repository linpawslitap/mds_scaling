\section{Design and implementation}

\begin{figure}[!hb]   %% START_FIGURE
\centerline{\includegraphics[scale=0.35]{./figs/giga-impl-leveldb-clusterfs}}
\vspace{10pt}
\caption{\textit{\footnotesize
Our scalable metadata service integrates two components: GIGA+ \cite{GIGA11},
a highly parallel and load-balanced indexing technique 
to partition metadata over multiple servers, and TableFS \cite{TableFS},
an log-structured (LevelDB) on-disk metadata representation on each server.
This integrated solution is layered on top of an existing cluster
file system deployment (PanFS) to improve metadata
and small file operation efficiency.
}}
%\vspace{10pt}
\hrule
\label{fig:design}
\end{figure}       %% END_FIGURE

Figure \ref{fig:design} presents the overall architecture of our scalable
metadata service. Our metadata service is a middleware inserted into
existing deployments of cluster file systems to improve metadata efficiency
while maintaining high I/O bandwidth for data transfers.
The system uses a client-server architecture,
and consists of three core components:

\begin{itemize}
\item{\textbf{Client:}} Applications interact with our middleware
through the FUSE user-level file system \cite{fuse},
through a library directly linked into the application
or through a module in a common library such as MPI-IO \cite{mpi}.
The stateless client-side code redirects applications' file operations
to the appropriate destination according to the types of operations.
All metadata requests (e.g. \texttt{create()} and \texttt{mkdir()}),
and data requests on small files (e.g. \texttt{read()} and \texttt{write()}),
are handled by the metadata indexing modules that address
these requests to the appropriate server.
For all data operations on large size files, the client code redirects
the request directly to the underlying cluster file system to take full
advantage of data I/O bandwidth. A newly created, but growing file
may be transparently reopened by the client module.

\item{\textbf{Metadata Indexing Server:}}
Each indexing server manages its local metadata storage backend to store and
access all metadata information and small file data.
It uses the GIGA+ algorithm to
partition large directories across indexing servers. It also monitors the growth
of small files, and migrates newly large files into
the underlying cluster file system
when its size exceeds a threshold.

\item{\textbf{Metadata Storage Backend:}}
The metadata storage backend is a modified version of \tfs which packs metadata
and small file data into large, log-structured, flat files,
and stores these files
in the underlying cluster file system. Since \tfs converts random updates into
sequential writes, it greatly improves disk performance. In order to
dynamically redistribute large directories,
the metadata storage backend also modifies
\tfs to support exporting and importing flat files in batch.

\end{itemize}

Remainder of this section describes more details of our system.
Section \ref{design.giga} presents a primer on how \giga distributes metadata.
Section \ref{design.tablefs} shows how \tfs stores all file system metadata
and small files using a single on-disk structure on each server.
Section \ref{design.integration} focus on the challenges in effectively
integrating \giga and \tfs to work with existing cluster file systems.


\subsection{Distributed Metadata Indexing}
\label{design.giga}
\input{giga}

\subsection{Client Caching Protocol}

\subsection{Metadata Storage Backend}
\label{design.tablefs}
\input{tablefs}

\subsection{Integrating \giga{} and \tfs{}}
\label{design.integration}
\input{integration}

