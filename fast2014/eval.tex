\section{Experimental Evaluation}

This section evaluates the scale and performance of our prototype using a mix of
metadata-intensive and data-intensive benchmarks. Our evaluation layers \sys on
the Panasas PanFS file system, and focuses on three questions:
\begin{itemize}
\item How does \sys enhance metadata throughput, particularly
number of file creates per second for concurrent file creation workloads, on
the PanFS storage system?
\item What is the data throughput of \sys layered on PanFS for N-N checkpointing 
workload found in HPC systems?
\item How portable is \sys to be used on other cluster storage systems and
configurations?
\end{itemize}

\subsection{Setup and Methodology}

Our prototype is implemented in about 10,000 lines of C code using
a modular design comprising of \tfs, \ldb and PanFS layers. In particular, the
PanFS layer is unmodified and can be replaced with any other cluster file
system. The current version implements most common POSIX file system operations 
except \texttt{hardlink}, \texttt{rename}, and \texttt{xattr} related operations.
The first two operations, \texttt{hardlink} and \texttt{rename}, are
particularly complicated because they need distributed transaction support for
correct and fault tolerant cross-server operations; this problem is beyond the
scope of this paper.

All experiments were performed on two clusters described in Table
\ref{tab:setting}. The first cluster is a 5-shelf PanFS storage cluster, and the 
second cluster is a 64-node setup to run \sys code and applications.
Table \ref{tab:setting} describes the hardware and software configuration of
these clusters.
The first cluster is used to evaluate the scale and performance of the
underlying cluster file system that uses our middleware.
The second cluster is used to demonstrate that our middleware 
solution can be layered on other file system deployments without any
configuration changes; this setup
emulates a case that each GIGA+ server runs in a different NFS node and manages 
its own \tfs instance locally to scale the metadata performance of NFS.
In all tests, the client uses library version code;
the threshold for splitting a partition is always 8,000 entries;
and \tfs managed by GIGA+ server syncs its data every 5 seconds.

\begin{footnotesize}
\begin{table}
\center
\begin{tabular}{lc}
\toprule
      & Cluster \\
\midrule
\#Nodes & 64 \\
\hline
OS &    Ubuntu 12.10 3.6.6 x86\_64 \\
\hline
CPU &  AMD Opteron 242 Dual Core\\
\hline
Memory & 16GB DDR-RAM\\
\hline
Network & 1GE NIC  \\
\hline
Storage & Western Digital hard disk \\
System & 2TB per node  \\
& 100 seeks/sec random seeks   \\
& 137.6 MB/sec sequential reads \\
& 135.4 MB/sec sequential writes \\
\bottomrule \\
\end{tabular}
\caption{
\textit{\footnotesize Settings of two clusters used for evaluation.}
}
\label{tab:setting}
\end{table}
\end{footnotesize}

In the following sections, we will first show the evaluation
on the end-to-end performance of the integrated system on top of PanFS,
and then present the results of a scaling experiment on another platform.

\subsection{End-to-end System Evaluation}
\label{sec:fullsystem}
\input{full}

\subsection{Understanding Scaling Behavior}

