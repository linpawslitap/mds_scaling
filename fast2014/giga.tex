\section{Metadata Distribution Mechanism}
Metadata operations, especially namespace management, are harder to scale
because of namespace semantics.
One challenging and interesting problem for metadata scaling is
how to partition metadata with balanced load across different servers.
The problem becomes more difficult for general file system operations
compared to object storage system,
because file system namespace is hierarchical and
some operations require accessing several items in the namespace.
We propose two mechanisms to distribute file system metadata:
1) \textit{dynamic namespace partitioning},
and 2) \textit{replicated namespace with shared files}.
The two mechanisms are based on different observations
made from previous workload analysis. In the following section,
we present the two mechanisms in detail, and discuss the design
trade-offs they made.

\subsection{Dynamic Namespace Partitioning}
We present a mechanism that utilize a dynamic
namespace partitioning strategy to distribute metadata.
The namespace partitioning is operated at the granularity of directory.
Each directory is assigned to an initial metadata server when it is created.
The initial assignment can be a randomly picked server or
a hashing of its unique identifier (inode number and its pathname).
Its file entires and their attributes are stored together within the same server.
Based on previous workload analysis, most directories are small.
Therefore storing file entries with its parent directory can preserve
locality that is good for $readdir()$ operations.
And random assignment of small directory can ensure the load balance
of directories across metadata servers.


On the other hand, there also exists a few very large directories.
To load balance these large directories, we adopt an distribute indexing mechanism
called \giga \cite{GIGA11}.
\giga incrementally splits a directory according to the directory size.
Each file entry and its associated attributes stored in a directory is
hashed and uniformly mapped into a hashing range using a distributed index.
When splitting a directory, \giga halves the hashing range,
migrate one half of the hash range as a new directory partition
into another metadata server.
Each directory partition can be further split if it grows
until the directory expands to all the metadata servers.

A core scalability idea in \giga is parallel splitting: each server splits
without system-wide serialization or synchronization.
Such uncoordinated growth causes \giga servers to have only a partial view of
the entire per-directory index;
there is no central server that holds the global view of the
partition-to-server mapping.
Each server knows about the partitions it stores and knows the
identity of other server that know more about each ``child'' partition
resulting from a prior split by this server.
This information is known as the per-server split history of
a directory's partitions.
The full per-directory \giga index is
a transitive closure of the split history on each
server and represents the lineage of the directory's partitioning.
More discussion of the cost-benefit of using
the lineage to locate directory entries is not relevant
to this work and can be found in prior \giga{} literature \cite{GIGA07, GIGA11}.

However, this distribution mechanism eliminates hierarchical locality to
some degree. To access a file or directory, it requires clients to
traverse ancestor directories that may be stored on different metadata servers.
To reduce the number of RPCs, clients can cache the information of
directories in the top part of the namespace.
Each cached directory entry has a time-based lease.
When the lease expired, client needs to contact the metadata server
to get updated information of the directory entry if needed.
Such lease-based caching mechanism will delay operations that
mutate the directory tree or permissions (e.g. $rename()$,
$setOwner()$ and $setPermission()$).
These operations need to wait until all clients' leases expire.

\subsection{Replicated Namespace with Sharded Files}

In order not to increase  latency and take advantage of in-memory metadata, we propose to replicate directories to all metadata servers so that each metadata server can resolve pathname locally. Files will be distributed to different metadata servers so file operations are load balanced. Communication with one metadata server is enough for most file operations because any metadata server contains all directories in the namespace. We define the metadata server which a file belongs to the primary server for this file. The primary server is decided by the sharding function known to all clients.

Though file operations can be completed with one metadata server, directories are created on all metadata servers. Without distributed transactions of file metadata operations, it leads to concurrency problems because clients can see inconsistent namespace.
we use fine-grained locking to support concurrent non-conflicting requests.
For a directory operation, a lock on the directory is equal to read locks from its parent to the root and write lock on the whole subtree rooting from this directory. Read locks can be shared among different clients while only one client can have the write lock. 
A lock on the file has the same meaning: read locks from parent to the root and write lock on the file.


