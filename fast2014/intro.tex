\section{Introduction}

Lack of a highly scalable and parallel metadata service is the
Achilles heel for many distributed file systems in both
the Data Intensive Computing (DISC) world \cite{HDFS}
and the High Performance Computing (HPC) world \cite{hpcs-io:2008, hecfsio:tr06}.
This is because most cluster file systems have used centralized, single-node
metadata management, and focused merely on scaling the data path,
i.e. providing high bandwidth parallel I/O to files that are gigabytes in size.

This inherent metadata scalability handicap limits numerous massively parallel
applications that produce workloads
requiring concurrent and high-performance metadata operations.
One such example, file-per-process checkpointing, requires the metadata service to
handle a huge number of file creates at the same time \cite{PLFS}.
Another example, storage management, produces a read-intensive metadata workload
that typically scans the metadata of the entire file system to perform
administration tasks \cite{filemgmt-ucsc, magellan-ucsc}.
Thirdly, even in the era of big data,
most things in even the largest cluster file systems are small
\cite{Dayal, brent13}.
Scalable storage systems should expect the numbers of small files stored
to soon achieve and exceed billions,
a known challenge for many existing cluster file systems \cite{GIGA11}.

We envision a scalable metadata middleware with
the goal -- \textit{evolution, not revolution} --,
that emphasizes the need for incremental improvements
to existing cluster file systems that lack a scalable metadata path.
Although newer cluster file systems, including Google's Colossus file system
\cite{50mfiles-in-googlefs:fikes10}, OrangeFS \cite{OrangeFS},
UCSC's Ceph \cite{ceph:weil06} and Copernicus \cite{sfs-ucsc},
promise an entirely new distributed metadata service,
it is undesirable to have to replace an existing cluster file system
running in a large production environment just
because their metadata path does not provide the desired scalability.
Existing cluster file system installations, such as
Hadoop HDFS \cite{HDFS} and Panasas PanFS \cite{PanFS}
can benefit from a solution that provides,
for instance, distributed directory support
that does not require any modifications to the running cluster file system.

To realize this goal, we first study studied multiple mechanisms to
load balance file system metadata across many servers.
These mechanisms explore different aspects of metadata workloads analysis
that we obtained by using real-world traces.
One mechanism is called dynamic namespace partition
which partitions the namespace at the granularity of directory,
and dynamically splits and migrates large directory partitions
to achieve load balance.
The other mechanism is to replicate directory tree over all
metadata servers and use hash mapping to shard all file metadata.
The two mechanisms make trade-offs on the performance of
various file systems operations and the complexity of concurrency
control.

Secondly, previous many file systems (such as Google FS
and HDFS) use in-memory metadata service which limit the number
of objects supported by the file system.
The need for a new representation of directory entries led us to develop
of a out-of-core metadata representation by using
Log-structure Merge tree (LSM-tree) \cite{ONeil1996}.
We use LSM tree approach to pack all file system metadata
(including directories, i-node attributes) and small files,
into many fewer, large, flat files.
This organization facilitates high-speed metadata creation, lookups and scans,
even in a single computer local disk configuration \cite{TableFS}.

To demonstrate the feasibility of our idea,
we implemented a prototype middleware service that
combines distribution mechanisms and on-disk metadata representation.
We evaluated the prototype on a Hadoop HDFS cluster \cite{HDFS} consisting of 256 
old machines with dual core CPU.
Our results show promising scalability and performance:
our solution layering on top of HDFS (\psys) can scale almost linearly
to 256 nodes and performs 2000 to 3000 operations per second
per node.
