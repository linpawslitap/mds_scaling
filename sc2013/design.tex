\section{Design and implementation}

\begin{figure}[t]   %% START_FIGURE
\centerline{\includegraphics[scale=0.3]{./figs/giga-impl-leveldb-clusterfs}}
\vspace{10pt}
\caption{\textit{
Our approach for a scalable metadata service integrates two components: a highly
parallel and load-balanced indexing technique \cite{GIGA11}
to partition metadata over multiple servers,
and an optimized metadata on-disk representation \cite{TableFS} on each server.
Our approach aims to layer this integrated solution on top of existing cluster
file system deployments to improve metadata and small file operation efficiency.
\kai{update graph}}}
\vspace{10pt}
\hrule
\label{fig:design}
\end{figure}       %% END_FIGURE

Figure \ref{fig:design} persents the overall architecture of our scalable
metadata service. Our metadata service is a middleware inserted into
existing deployments of cluster file systems to improve metadata efficiency
while maintaining high I/O bandwidth for data transferring.
The system uses a client-server architecture,
and consists of three core components:

\begin{itemize}
\item{\textbf{Client:}} Applications interact with our middleware
using posix-similar library interface, or the VFS interface exposed
through the FUSE user-level file system \cite{fuse}.
The stateless client-side code redirects applications' file operations
to approriate destinations according to the types of operations.
All metadata requests (e.g. \texttt{create()} and \texttt{mkdir()}),
and data requests to small files (e.g. \texttt{read()} and \texttt{write()}),
are handled through the metadata indexing modules that address
requests to the appropriate metadata indexing server.
For all data operations to large size files, the client code forwards
them directly to the underlying cluster file system to take full
advantage of data I/O bandwidth.

\item{\textbf{Metadata Indexing Server:}}
Each indexing server manages its local metadata storage backend to store and
access all metadata information and small file data. It uses GIGA+ algorithm to
partition large directory across indexing servers. It also monitors the growth
of small files, and migrates files into the underlying cluster file system
when their sizes exceed the threshold.

\item{\textbf{Metadata Storage Backend:}}
Metadata storage backend is a modified version of \tfs which packs metadata and
small file data into flat files, and stores flat files
in the underlying cluster file system. Since \tfs converts random updates into
sequential writes, it greatly improves metadata peformance. In order to
dynamcially distribute large directories, metadata storage backend also modifies
\tfs to support exporting and importing flat files in a batch.

\end{itemize}


Using \giga and \tfs enables us to tackle two key challenges: highly
concurrent metadata distribution for ingest-intensive parallel applications
such as checkpointing \cite{PLFS} and
optimized metadata representation that stores all file system
metadata in structured, indexed files managed by existing cluster file system
deployments.

Remainder of this section describes more details of our system.
Section \ref{design.giga} presents a primer on how \giga distributes metadata.
Section \ref{design.tablefs} shows how \tfs stores all file system metadata
and small files using a single on-disk structure on each server.
Section \ref{design.integration} focus on the challenges in effectively
integrating \giga and \tfs to work with existing cluster file systems.


\subsection{Distributed Metadata Indexing}
\label{design.giga}
\input{giga}

\subsection{Metadata Storage Backend}
\label{design.tablefs}
\input{tablefs}

\subsection{Integrating \giga{} and \tfs{}}
\label{design.integration}

To effectively build a middleware that integrates
the \giga distribution mechanism and \tfs,
we have to tackle two main challenges:
one is to modify \tfs to support for
migrating directory partition required by \giga;
the other is to decouple metadata and data paths to
achieve high peformance for both paths.
This section discusses the modifications we made to
overcome the two challenges.


~\\
\textbf{Metadata representation -- }
\tfs stores all metadata including \giga hash
partitions for directories, entries in each hash partition, and other
bootstrapping information such as root entry and \giga configuration state.
The general schema used to store all file is:

\begin{table}[!htc]
\begin{tabular}{c|c}
key & \texttt{parentDirID,gigaPartitionID,hash(dirEntry)} \\
\midrule \\
value & \texttt{attr(dirEntry),[symlink|data|gigaMetaState]} \\
\end{tabular}
\label{tab:keyschema}
\end{table}

The main difference from the \tfs schema described in Section
\ref{design.tablefs} is the addition of two \giga specific fields:
\texttt{gigaPartitionID} to identify a
\giga hash partition and \texttt{gigaMetaState} to store the
hash partition related mapping information for directories.
These \giga related fields are used only
if large directories are distributed over multiple metadata servers.
A optimization is to eliminate \texttt{gigaPartitionID} in the key
by using the same hash function for both \giga and \tfs keys,
since the hash of entry name can determine the partition ID.

~\\
\textbf{Partition splitting -- }
The local \tfs instance stores \giga hash partitions and their directory
entries as SSTable Files in the underlying cluster file system.
Recall that each \giga server process splits a hash partition $P$ on
overflow and creates another hash partition $P'$ which is managed by a
different server; this split involves migrating approximately half the entries
from old partition $P$ to the new hash partition $P'$ on another server.
During splitting, the partition in migratoin has to be locked from
concurrent accessing for correctness.
We explored several ways to perform this cross-server partition split.

A straightforward solution would be to perform a range scan on
partition $P$, and remove about half the entries
(that will be migrated to the new partition $P'$) from $P$.
All removed entries are batched together
and sent in an RPC message to the server that will manage partition $P'$.
The split receiver inserts each key in the batch into its own \tfs instance.
While simplicity of this solution makes it attractive,
it is slow in practice and vulnerable to failures during splitting.
We have to devise a faster and safer technique to
reduce the time that the splitting range is in lock.

The immutability of SSTables in \ldb makes such a fast bulk insert possible --
an SSTable can be added to Level 0 without its data being pushed through the
write-ahead log and minor compaction process.
To take advantage of this opportunity, we extended \tfs
to support a three-phase split operation:

\begin{itemize}
\item{Phase 1:} The split initiator performs a range scan on its \tfs instance
to find all entries in the hash-range that needs to be moved to another server.
Instead of packed into an RPC message,
the results of this scan are written in SSTable format to files in the
underlying cluster file system.

\item{Phase 2:} The split initiator notifies the split receiver about
the paths to the SSTable-format files in a much smaller RPC message.
Since these files are stored in shared storage,
the split receiver directly inserts these files as symbolic links
into the \ldb tree structure without actually copying these files.

\item{Phase 3:} The final step is a clean-up and commit phase:
after the receiver completes the bulk insert operation, it notifies the
initiator, who then deletes the migrated hash-range from its \tfs instance
and unlocks the range.
\end{itemize}


~\\
\textbf{Decoupled data and metadata path -- }
All metadata operations go through the \giga{} server; however, following the
same path for data operations would incur an unnecessary performance penalty
of shipping data over the network on extra time.
This penalty can be significant in HPC use-cases where files can easily be
gigabytes to terabytes in size.

To avoid this penalty our middleware is designed to perform all
data-path operations directly through the cluster file system module in client
machine. 
Figure \ref{fig:design} illustrates this data path (in BLUE color).
Once the client completes a
lookup on a desired file name, it gets back a symbolic link to the physical
path in the cluster file system. All subsequent accesses using this symbolic
link force the client operating system to resolve this link into the cluster
file system.
While the file is open, some of its attributes (e.g., file size and last access time)
may change relative to \ldb's per-open
copy of the attributes. \giga will capture these changes on file close on the
metadata path. Other attribute changes relatvie to permissions can be updated on-flight
through \giga servers.

%Tricks in client side

