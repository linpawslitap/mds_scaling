\section{Experimental Evaluation}

Using metadata-intensive and data-intensive benchmarks,
we evaluate our middleware's overall performance
and explore how its system design contribute to meeting its goals.
We specifically examine: (1) its improvement on the scalability and
efficiency of metadata service for cluster file systems,
especially in concurrent file creation workloads;
(2) the data throughput in N-N checkpointing workloads
when layering our middleware on top of PanFS.

\textbf{Implementation}
Our prototype is implemented in 14K lines of C using a modular design.
\tfs, \ldb and PanFS are modular plugins that can be replaced by other backends
that follow the API semantics. The current version implements most posix
file system operations except \texttt{hardlink}, \texttt{rename},
and operations for extended attributes.

\textbf{Evaluation System }
We evaluate the overall performance of our prototype on
a 5-shelves PanFS cluster with five 64-core test nodes. The detailed hardware
and software configuration is shown in Table \ref{tab:setting}.
We also used another cluster to demonstrate that our middleware solution
can be layered on other file system deployments as well.
The second cluster setting emulates a case that each GIGA+ server runs in
a different NFS node and manages its own \tfs instance locally
to scale the metadata performance of NFS.
In all tests, the client uses library version code;
the threshold for splitting a partition is always 8,000 entries;
and \tfs managed by GIGA+ server syncs its data every 5 seconds.

\begin{table}
\begin{tabular}{lcc}
\toprule
      & Cluster 1 & Cluster 2 \\
\midrule
\#Nodes & 5 & 64 \\
\hline
OS &   CentOS 6.3 &  Ubuntu 12.10 \\
Kernel & 2.6.32 x86\_64 & 3.6.6 x86\_64 \\
\hline
CPU & AMD Opteron 6272 &  AMD Opteron 242 \\
    & 64 Cores & Dual Core\\
\hline
Memory & 128GB DDR &  16GB DDR \\
\hline
Network &       &           \\
\hline
Storage & PanFS & Western Digital \\
System &      5 Shelves & Local hard disk  \\
       &   5MDS + 50 ODS &  2TB per node  \\
& & 100 seeks/sec \\
& & rand. seeks   \\
& & 137.6 MB/sec  \\
& & seq. reads    \\
& & 135.4 MB/sec  \\
& & seq. writes   \\
\bottomrule \\
\end{tabular}
\caption{
\textit{The settings of two clusters for evaluation.}
}
\label{tab:setting}
\end{table}

In the following sections, we will first show the evaluation
on the end-to-end performance of the integrated system on top of PanFS,
and then present the results of a strong scaling experiment on another platform. 

\subsection{Full System Benchmark}
\label{sec:fullsystem}
\input{full}

\subsection{Strong Scaling Experiment}
Next, we evaluated the scalability of our distributed metadata middleware prototype.
The scalability experiment was conducted in another 64-nodes cluster.
Each node had a GIGA+ indexing server process that manages its own \tfs
instance whose SSTables are stored on a local disk running Ext4 file system.
To emulate shared storage for split operations,
we used a NFS-mounted directory accessible from all machines;
this shared directory was only used for moving SStables of
splitting directory partitions across servers.

The workload we used is a \textit{strong scaling} experiment, i.e.
creating 1 million files per server, for a total of 64 million files in the
64-server configuration. We varies the number of servers from 1 to 64
to see how the performance of the integration scale with more machine power.

Figure \ref{graph:ldb-scaling} shows the instantaneous throughput
during the concurrent create workload.
The main result in this figure is that as the number of servers doubles the
throughput of the system also scales up. With 64 servers, \giga can achieve a
peak throughput of about 190,000 file creates per second.
The prototype delivers peak performance after the directory workload
has been spread among all servers.
Reaching steady-state, the throughput quickly grows
due to the splitting policies adopted by \giga.

After reaching the steady state, throughput slowly drops
as \tfs builds a larger metadata store.
In fact, in large setups with 8 or more servers,
the peak throughput drops by as much as 25\% (in case of the 64-server setup).
This is because when there are more entries already existing in \tfs,
it requires more compaction work to maintain invariants inside \ldb
and to perform a negative lookup before each create
has to search more SSTables on disk.
In theory, the work of inserting a new entry to a LSM-tree is $O(\log_{B}(n))$
where $n$ is the total number of inserted entries, and $B$ is a constant factor
proportional to the average number of entries transferred in each disk request
\cite{Bender2007}.
Thus we can use the formula $\frac{a\cdot S+b}{\log{T}}$ to
approximate the throughput timeline in Figure \ref{graph:ldb-scaling},
where $S$ is the number of servers, $T$ is the running time,
and $a$ as well as $b$ are constant factors
relative to the disk speed and splitting overhead.
This estimation projects that when inserting 64 billion files with 64 servers,
the system may deliver an average of 1,000 operations per second per server,
i.e. 64,000 operations per second in aggregate.

\begin{figure*}[t]
\centerline{\includegraphics[scale=0.65]{./figs/ldb_insertrate}}
\vspace{10pt}
\caption{\normalsize
\textit{Our middleware metadata service prototype shows promising scalability
up to 64 servers.
Note that at the end of the experiment,
the throughput drops to zero
because clients stop creating files as they finish 1 million files per client.
And the solid lines in each configuration are Bezier
curves to smooth the variability.}
}
\vspace{10pt}
\hrule
\label{graph:ldb-scaling}
\end{figure*}


