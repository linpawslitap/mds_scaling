We performed an end-to-end evaluation of our prototype in our second cluster.
This cluster has 5 test nodes, and is connected to
a 5-shelves PanFS storage cluster with 5 metadata nodes and 50 storage nodes.
Each test node has 64 cores and is using 40GE NIC, which are enough
to saturate the data bandwidth of our PanFS storage cluster.
Because of some technique difficulties,
we did not run our GIGA+ server processes inside the metadata node.
Instead, we co-locate our GIGA+ server processes
with client processes in the test nodes.
Each test node runs a GIGA+ server that is assigned to a metadata node
as explained in Section \ref{design.integration}.
We ran a series of HPC benchmark runs using the open source \textit{mdtest}
synthetic benchmark \cite{mdtest}
and File System Test Suite checkpoint benchmark from LANL \cite{mpiio}
to test metadata path and data path separately.

\textbf{Metadata Intensive Workloads}
To evaluate the metadata performance of layering on PanFS,
we use mdtest to generate a three-phase workload:
The first phase, similar to the last one, is to create 5 million
zero-files in a single directory;
the second phase is to perform a $stat()$ on random files in the directory;
the third phase is to delete all the files in the directory in a random order.
Each phase involves multiple clients to issue the operations concurrently.

If we directly use the above workload to directly compare our layered system
against the original PanFS, it would not be fair enough.
This is because a single directory can only use the hardware resource
of one metadata manager in PanFS,
and PanFS also limits a single directory to 1 million files.
Therefore we chose to compare native PanFS creating 1 million files
in 5 different directories owned by 5 different metadata managers.
The total number of clients used for testing the two systems
are kept the same.


\begin{figure}[t]  %%%%%%%%%%%%%%%%%%%%%%%
\centerline{\includegraphics[scale=0.7]{./figs/zero_file_creation_on_panfs}}
\vspace{10pt}
\caption{\normalsize
\textit{Average throughput during creating five million zero-length files
in one empty directory with different number of clients per test node.}
}
\vspace{10pt}
\hrule
\label{graph:creation_clients}
\end{figure}       %%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]  %%%%%%%%%%%%%%%%%%%%%%%
\centerline{\includegraphics[scale=0.7]{./figs/mdtest}}
\vspace{10pt}
\caption{\normalsize
\textit{mdtest:
The average aggregated throughput of different operations in mdtest
when generating 5 million zero-length files in a single shared directory.
Since PanFS has a hard limit to allow only create 1 million entries
in one directory, the bar showing PanFS with 1 volume only gives
the average throughput for the case of creating 1 million entries.
}
}
\vspace{10pt}
\hrule
\label{graph:mdtest_ops}
\end{figure}       %%%%%%%%%%%%%%%%%%%%%%%


Figure \ref{graph:creation_clients} shows the aggregated througphut during
the first phase. We varies the nubmer of clients running in each test node
to find the right number that can saturate both systems.
Both systems achieve the highest aggregated throughput when the number
of clients per node is 32 or more. In all the experiments shown later,
we present the results with 32 clients per test node if without explanation.

For all cases, \sys  is approximately 10 times faster than the native PanFS
using 5 volumes. The aggregated throughput with 32 clients per server
achieves about 24,571 creation per second, which is also faster than
the case of using 8 servers in previous experiment. Using PanFS as storage
backends for SSTables provides higher bandwidth than the local disks in
the first cluster.

Figure \ref{graph:mdtest_ops} shows the aggregated througphut of
different operations during three phases in mdtest.
Besides \sys and PanFS using 5 volumes, it also shows
the aggregated throughput when creating 1 million files in one volume of PanFS.
For $lookup$ and $deletion$ workloads,
\sys gains more advantages over original PanFS compared to file creation.
Fast lookup is due to the memory indexing and Bloom filters in \ldb.
For deletion of a key, \ldb essentially just inserts the key and a deletion mark
into its memtable, and delay the actual deletion in later compaction processes.


\textbf{Small File Workloads}
We also use mdtest benchmark to generate files with small size data to evaluate
the effectiveness of embedding file content with the metadata inside \tfs.
Similarly mdtest benchmark creates 5 million files in a single directory
but with two different sizes of data: 4KB and 16 KB.
4KB is the median file size for many desktop workloads \cite{Bill11},
and 16KB is the median file size for some large storage clusters \cite{brent13}.

%compaction and CPU usage
Figure \ref{graph:smallfiles} shows the aggregated throughput during test
that is aggregated from all clients. For 4KB file size,
\sys is about $2.5\times$ faster than original PanFS.
However, \sys is about $35\%$ slower than PanFS for 16KB file size.
We found that embedding 16KB file makes the key-value pair significanly larger,
and causes higher write amplification during compaction process in \ldb,
since \ldb tries to merge sort both file data and metadata.
Additionally, \ldb will firstly write the inserted key-value pairs
to the transaction log, and then (during a compaction) to the SSTable.
With larger key-value pairs, LevelDB's per-operation efficiency is
slowed down by the cost of extra copies of large values.

%mention tuning?

Alhough embedding small files beyond 4KB help with
future file reads by reducing seeks, it hampers fast insertion workload.
We might sacrafice the read performance while maitaining fast insertion,
by using column-based approach to store metadata and small file separately.
Such investigation is left for future works.

\begin{figure}[t]  %%%%%%%%%%%%%%%%%%%%%%%
\centerline{\includegraphics[scale=0.7]{./figs/small_file_creates}}
\vspace{10pt}
\caption{\normalsize
\textit{Average aggregated throughput during creating 5 million small files
with different size in one shared directory}
}
\vspace{10pt}
\hrule
\label{graph:smallfiles}
\end{figure}       %%%%%%%%%%%%%%%%%%%%%%%


%Library version not FUSE
%Clean cache

\textbf{Data Intensive Workloads}
The LANL filesystem checkpoint benchmark can
generate many types for HPC checkpoint I/O patterns.
For all of our tests, we configured the benchmark to generate a workload
that multiple processes concurrently write N-N checkpoint to the file system.
All checkpoint file I/O is performed by a set of processes
that synchronize with each other using MPI barriers.
At the begining, each process opens a freshly created checkpoint file
for writing and then waits at a barrier until all processes are ready to write.
Once all processes are ready, each processes starts
concurrently writing the checkpoint data to its own file,
until it has written the specified number of access units.
It then waits barrier for all the other processes to finish writing,
and finally syncs its data to disk and closes the file.
Before starting the read phase we terminate all processes
accessing these checkpoint files so that
we can unmount the filesystem in order to ensure that
all freshly written data has been flushed from all the nodes'
memory to avoid cached data from unfairly biasing our read performance.
After the filesystem has been mounted and restarted,
the benchmark reads the checkpoint in the same way it was written,
however we shift, so each process will read
the file generated by another process.

\begin{figure}[t]  %%%%%%%%%%%%%%%%%%%%%%%
\centerline{\includegraphics[scale=0.7]{./figs/checkpointing_write}}
\vspace{10pt}
\caption{\normalsize
\textit{
The aggregated write throughput in N-N check-pointing workload.
Each volume receives 640 GB data.
}
}
\vspace{10pt}
\hrule
\label{graph:checkpoint_write}
\end{figure}       %%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]  %%%%%%%%%%%%%%%%%%%%%%%
\centerline{\includegraphics[scale=0.7]{./figs/checkpointing_read}}
\vspace{10pt}
\caption{\normalsize
\textit{
The aggregated read throughput in N-N check-pointing workload.
}
}
\vspace{10pt}
\hrule
\label{graph:checkpoint_read}
\end{figure}       %%%%%%%%%%%%%%%%%%%%%%%

