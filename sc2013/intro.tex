\section{Introduction}
Lack of a highly scalable and parallel metadata service is the
Achilles heel for many parallel and cluster file systems in both the HPC world
\cite{hpcs-io:2008, hecfsio:tr06} and the Internet services world \cite{HDFS}.
This is because most cluster file systems have used centralized, single-node
metadata management, and focused merely on scaling the data path,
i.e. providing high bandwidth parallel I/O to files that are gigabytes in size.


This inherent metadata scalability handicap limits numerous massively parallel
applications that produce workloads
requiring concurrent and high-performance metadata operations.
One such example, file-per-process checkpointing, requires the metadata service to
handle a huge number of file creates at the same time \cite{PLFS}.
Another example, storage management, produces a read-intensive metadata workload
that typically scans the metadata of the entire file system to perform
administration tasks \cite{filemgmt-ucsc, magellan-ucsc}.
Thirdly, even in the era of big data,
most things in even the largest cluster file systems are small
\cite{Dayal, brent13}.
Scalable storage systems should expect the numbers of small files stored
to soon achieve and exceed billions,
a known challenge for many existing cluster file systems \cite{GIGA11}.

We envision a scalable metadata service with two goals.
The first goal -- \textit{evolution, not revolution} -- emphasizes the need for
incremental improvements to existing cluster file systems that lack a
scalable metadata path.
Although newer cluster file systems, including Google's Colossus file system
\cite{50mfiles-in-googlefs:fikes10}, OrangeFS \cite{OrangeFS}, UCSC's Ceph \cite{ceph:weil06} and 
Copernicus \cite{sfs-ucsc}, promise an entirely new distributed metadata
service, it is undesirable to have to replace an existing cluster file system
running in a large production environment
just because their metadata path does not provide the desired scalability.
Several large cluster file system installations, such as Panasas PanFS running
at LANL \cite{PanFS} and Lustre running at Oak Ridge
\cite{shipman2010, wang09}, can benefit from a solution that provides,
for instance, distributed directory support
that does not require any modifications to the running cluster file system.

The second goal -- \textit{generality and de-specialization} -- promises a
fully, distributed and scalable metadata service
that performs well for creates, deletes, lookups, and scans.
In particular, all metadata, including directory entries, i-nodes and block
management, should be stored in one structure; this is different from
today's file systems that use specialized on-disk structures for each type of
metadata.

To realize these goals, this paper makes a case for a scalable metadata
middleware service \sys that layers on top of existing cluster file systems and
load balance file system metadata, including the namespace, small
directories and large directories, across many servers.
Our key idea is to effectively integrate a concurrent indexing
technique to distribute metadata with a tabular, log-structured on-disk
representation of all file system metadata.

For distributed indexing, we re-use the concurrent, incremental, hash-based
\giga indexing technique \cite{GIGA11}.
The main shortcoming of the original \giga prototype was the cost of splitting
metadata partitions for better load-balancing. This required migrating
directory entries and associated file data \cite{GIGA11} from one storage
server to another.
This is inefficient for HPC systems where files can be gigabytes or more in
size. Our \sys avoids this data migration by interpreting directory
entries as symbolic links: each directory entry (the name created by the
application) has a physical pathname that points to a file
with the actual file content stored in the underlying cluster file system.

The need for a new representation of directory entries led us to develop
of a novel on-disk metadata representation called \tfs \cite{TableFS},
based on a log-structure merge tree (LSM-tree) data structure \cite{ONeil1996}.
We use the \tfs approach to pack all file system metadata
(including directories, i-node attributes) and small files,
into many fewer, large, flat files.
This organization facilitates high-speed metadata creation, lookups and scans,
even in a single computer local disk configuration \cite{TableFS}.

Effectively integrating the \tfs metadata store with the \giga distributed indexing
technique requires several optimizations including cross-server split operations
with minimum data migration, and decoupling data and metadata paths.
To demonstrate the feasibility of our approach,
we implemented a prototype middleware layer \sys and evaluated it
on an existing Panasas PanFS deployment \cite{PanFS} that has 5 shelves
consisting of 5 metadata servers and 50 data servers.
We called the combined solution \psys.
Our results show promising scalability and performance:
\sys layering on top of PanFS (\psys) was more than 10$\times$ faster than the original PanFS
for metadata intensive workloads, and performs comparably for data-intensive workloads.
