\section{Related Work}
\label{relatedwork}

This paper proposes a layered approach that allows a cluster file system to 
distribute both namespace and directories. In this section, we discuss prior
work related to metadata systems in modern cluster file systems and optimized
layouts for high-performance metadata.

In many cluster file systems, the metadata path lacks the concurrency and 
scalability found in the data path.
Many file systems, including Lustre \cite{lustre}, Google file system
\cite{gfs:ghemawat03} and HDFS \cite{HDFS}, continue to rely on a single
metadata server to store all file system metadata. This simplifies the
complexity of administration and implementation, but limits the scalability to 
how fast one server can service all metadata operations. 
Some cluster file systems distribute metadata on multiple metadata servers. The 
main difference is what metadata is distributed and how it is distributed.

PanFS and PVFS cluster distribute different parts of the namespace on different
metadata servers.
PanFS uses a coarse-grained distribution by assigning a subtree of the
namespace (called volume) to each metadata server (called directory blade) 
\cite{panfs:welch08}.
PVFS is more fine-grained: it spreads different directories, even the ones in
the same sub-tree, on different metadata servers \cite{pvfs}.
However, both PanFS and PVFS lack support for distributed directories.

Few cluster file systems have added support for distributed directories but
each system uses a different technique to spread these directories.
A beta release of OrangeFS, a commercially supported PVFS distribution, uses a 
simplified version of GIGA+ to distribute large directories on several metadata 
servers \cite{OrangeFS}.
Ceph uses an adaptive partitioning technique called CRUSH for distributing its
metadata and directories on multiple metadata servers \cite{ceph:weil06}. While
experimental version of Ceph (from 2006) shows promising directory scalability,
the recent versions of Ceph directory clustering have been described as
"less stable" \cite{ceph-baddirs1:www} and "buggy" \cite{ceph-baddirs2:www}.
IBM GPFS uses extensible hashing to distribute directories on different disks on 
a shared disk subsystem \cite{gpfs:schmuck02}. GPFS has the most complete and
stable distributed directory implementation, but its design relies heavily on
cache consistency and distributed locking mechanisms. When compared to GIGA+,
this may result is much lower file create performance (when files are created
in a single directory) but GPFS delivers very high directory read performance 
by caching directory blocks on all readers \cite{GIGA11}.

The second aspect of this paper is the use of a optimized single-node metadata
representation that enables high throughput (metadata operations per second).
Several recent efforts have focused on using advanced data-structures, such
as modified LSM-trees, stratified B-trees and fractal trees, to speed up number
of IOs per second \cite{blsm, tokufs, Andrew11}. 

