Our metadata storage backend implements a modified version of \tfs
to pack together and manage all the metadata and small files,
hiding them from the underlying cluster file system.

\tfs \cite{TableFS} is a stacked file system which uses another file system
as an object store, and organizes all metadata and small files into a single
on-disk table using a Log-Structured Merge (LSM) tree \cite{ONeil1996, LevelDB}.
The reason for using LSM tree is that it buffers new and changed entries in
memory and translate small random disk writes into large sequential writes.
Because LSM tree can dramatically reduce random disk seeks,
it is a natural fit for metadata intensive workloads.
We decribe the structure of an LSM tree
and how LSM trees are used in \tfs in greater detail
in the following sections.

%\begin{figure}[!ht]
\begin{figure}[t]
\includegraphics[scale=0.4]{figs/leveldb}
\vspace{10pt}
\caption{\textit{\footnotesize
LevelDB is an on-disk LSM-tree implementation that represents data in multiple 
files (called SSTables) containing sorted key-value pairs.
SSTables are grouped into different levels with lower-numbered levels
containing more recently inserted key-value pairs.
Finding a specific pair on disk may search up to all SSTables in level-0
and at most one in each higher-numbered level.
Compaction is the process of combining SSTables
by merge sort and moving combined SSTables into higher-numbered levels.
}}
%\vspace{10pt}
\hrule
\label{fig:leveldb}
\end{figure}


\textbf{LSM trees and LevelDB --}
\tfs uses an open-source implementation of an LSM tree called LevelDB
\cite{LevelDB}. LevelDB provides a simple key-value store interface,
supporting point queries and range queries. In LevelDB, by default,
a set of changes are spilled to disk when the total size of modified
entries exceeds 4 MB.  When a spill is triggered, called a
minor compaction, the changed entries are sorted, indexed and written to disk
in a format called an SSTable \cite{BigTable}.  These entries may then be
discarded from the in memory buffer. Discarded entries
can be reloaded by searching each SSTable
on disk, possibly stopping when the first match occurs if the SSTables are
searched most recent to oldest.  The number of SSTables that need to be
searched is reduced by maintaining the minimum and maximum key value
and a Bloom filter\cite{bloomfilter} on each,
but, with time, the cost of finding a record not in memory still increases.
Major compaction, or simply ``compaction",
is the process of combining multiple overlapping range SSTables
into a number of disjoint range SSTables by merge sort.

As illustrated in Figure \ref{fig:leveldb},
LevelDB extends this simple approach to further
reduce read costs by dividing SSTables into levels.
In 0-th level, each SSTable may contain entries with any key value,
based on what was in memory at the time of its spill.
The higher-numbered levels of LevelDB's SSTables are
the results of compacting SSTables from their own or lower-numbered levels.
In levels excepth the 0-th level, LevelDB maintains the following invariant:
the key range spanning each SSTable is disjoint from
the key range of all other SSTables at that level.
So querying for an entry in the higher levels
only needs to read at most one SSTable in each level.
LevelDB also sizes each of the higher levels differentially:
all SSTables have the same maximum size and
the sum of the sizes of all SSTables at level $L$ will not exceed $10^L$ MB.
This ensures that the number of level grows
logarithmically with increasing numbers of entries.

~\\
\textbf{Table schema -- }
\tfs (prior to \sys) aggregates directory entries,
i-node attributes and small files into one LSM tree
with an entry for each file and directory.
To translate the hierarchical structure of the file system namespace
into key-value pairs, the 224-bit key is chosen to consist of
the 64-bit i-node number of a entry's parent directory
and a 160-bit SHA-1 hash value of its filename string
(final component of its pathname).
The value of an entry contains the file's full name and i-node attributes,
such as i-node number, ownership, access mode, file size, timestamps (\textit{struct stat} in Linux).
For small files with size less than $T$ (defaulting to 4KB),
the value field also contains the file's data.
For large files, the file data field in a file row of the table
is replaced by a symbolic link that points to
the actual file object in the underlying file system.

%\begin{figure}[!ht]
\begin{figure}[t]
\centering
\includegraphics[scale=0.4]{figs/schema}
\vspace{10pt}
\caption{\textit{\footnotesize
An example illustrating a table schema for \tfs
to store metadata and file data as key-value pairs.}}
%\vspace{10pt}
\hrule
\label{fig:schema}
\end{figure}

Figure \ref{fig:schema} shows an example of representing
a sample file system's metadata into one table.
Embedding the i-node number of parent directory into the key
helps with resolving the pathname.
Traversing the user's directory tree
involves constructing a search key by concatenating the i-node
number of current directory with the hash of
next component name in the pathname.
Another benefit of this schema is that all the entries
in the same directory have rows that
share the same first 64 bits in their row key.
For $readdir$ operations, once the i-node number
of the target directory has been retrieved,
a scan sequentially lists all entries having
the directory's i-node number as the first 64 bits of their table's key.

Previous evaluation \cite{TableFS} has shown using
the \tfs schema with a \ldb backend
can greatly improve metadata performance of a local file system.
Figure \ref{graph:ldb-singlenode} is taken from a prior \tfs paper,
which compared the instantaneous single disk throughput of a FUSE-based \tfs
with three underlying Linux file systems: Ext4 \cite{Ext4}, XFS \cite{XFS}, and
Btrfs \cite{BTRFS}.
The workload created 100 million zero-length files in a single directory
on one disk.
All systems perform well at the very beginning of the test, but the file create
throughput drops significantly for all systems as the total disk gets larger.
Btrfs suffers the most serious throughput drop, slowing down to 100 operations
per second.
\tfs, however, maintains a more steady performance
with an average speed of 2,200 operations per second respectively,
\textit{and is 10X faster than all other tested file systems.}

\begin{figure}[t]  %%%%%%%%%%%%%%%%%%%%%%%
\centerline{\includegraphics[scale=0.5]{./figs/ldb_insertrate_onenode}}
\vspace{10pt}
\caption{\textit{\footnotesize
This graph (from prior work \cite{TableFS}) shows
that the performance of single-node \tfs with FUSE is 10X faster than modern Linux
filesystems. The workload is to create 100 million zero-length files in one directory.
X-axis only shows the time until \tfs finished all insertions because the other
file systems were much slower. Y-axis has a logarithmic scale.}
}
%\vspace{10pt}
\hrule 
\label{graph:ldb-singlenode}
\end{figure}       %%%%%%%%%%%%%%%%%%%%%%%
